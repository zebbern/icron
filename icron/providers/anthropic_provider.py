"""Native Anthropic provider implementation with full API support."""

import asyncio
import logging
from typing import Any

import anthropic
from anthropic import AsyncAnthropic, APITimeoutError, APIError, RateLimitError, AuthenticationError

from icron.providers.base import LLMProvider, LLMResponse, ToolCallRequest

logger = logging.getLogger(__name__)

# Default timeout for API requests (seconds)
DEFAULT_TIMEOUT = 120


class AnthropicProvider(LLMProvider):
    """
    Native Anthropic provider with full API support.

    Supports:
    - Extended thinking (Claude 3.7 Sonnet+)
    - Prompt caching headers
    - Top-k sampling
    - Custom base URL (for compatible endpoints)
    - Tool calling

    Automatically handles retries for rate limits and transient errors.
    """

    def __init__(
        self,
        api_key: str | None = None,
        api_base: str | None = None,
        default_model: str = "claude-sonnet-4-20250514",
        timeout: int = DEFAULT_TIMEOUT,
    ):
        """
        Initialize Anthropic provider.

        Args:
            api_key: Anthropic API key.
            api_base: Custom base URL for compatible endpoints (e.g., Zhipu CodePlan).
            default_model: Default model to use.
            timeout: Request timeout in seconds.
        """
        super().__init__(api_key, api_base)
        self.default_model = default_model
        self.timeout = timeout

        # Initialize async client
        client_kwargs: dict[str, Any] = {
            "timeout": timeout,
        }
        if api_key:
            client_kwargs["api_key"] = api_key
        if api_base:
            client_kwargs["base_url"] = api_base

        self.client = AsyncAnthropic(**client_kwargs)

    async def chat(
        self,
        messages: list[dict[str, Any]],
        tools: list[dict[str, Any]] | None = None,
        model: str | None = None,
        max_tokens: int = 4096,
        temperature: float = 0.7,
        top_k: int | None = None,
        thinking: dict[str, Any] | None = None,
        system: str | None = None,
        enable_cache_headers: bool = False,
        **kwargs: Any,
    ) -> LLMResponse:
        """
        Send a chat completion request via Anthropic API.

        Args:
            messages: List of message dicts with 'role' and 'content'.
            tools: Optional list of tool definitions in OpenAI format (auto-converted).
            model: Model identifier (e.g., 'claude-sonnet-4-20250514').
            max_tokens: Maximum tokens in response.
            temperature: Sampling temperature (0-1).
            top_k: Anthropic-specific top-k sampling.
            thinking: Extended thinking config, e.g. {"type": "enabled", "budget_tokens": 10000}
            system: System prompt (overrides any in messages).
            enable_cache_headers: Enable prompt caching with cache_control headers.
            **kwargs: Additional provider-specific parameters.

        Returns:
            LLMResponse with content and/or tool calls.
        """
        model = model or self.default_model
        
        # Strip the @anthropic/ prefix if present (used for provider routing)
        if model.startswith("@anthropic/"):
            model = model[len("@anthropic/"):]

        # Convert OpenAI-style tools to Anthropic format
        anthropic_tools = None
        if tools:
            anthropic_tools = self._convert_tools_to_anthropic(tools)

        # Prepare messages (convert format if needed)
        anthropic_messages = self._convert_messages_to_anthropic(messages)
        
        # DEBUG: Log the converted messages
        logger.debug(f"[DEBUG] Anthropic: Original messages count: {len(messages)}")
        for i, m in enumerate(messages):
            role = m.get('role', 'unknown')
            tool_calls = m.get('tool_calls', [])
            tool_call_id = m.get('tool_call_id')
            logger.debug(f"  Original [{i}] role={role}, tool_calls={len(tool_calls) if tool_calls else 0}, tool_call_id={tool_call_id}")
        
        logger.debug(f"[DEBUG] Anthropic: Converted messages count: {len(anthropic_messages)}")
        for i, m in enumerate(anthropic_messages):
            role = m.get('role', 'unknown')
            content = m.get('content')
            if isinstance(content, list):
                types = [c.get('type', 'unknown') for c in content]
                logger.debug(f"  Converted [{i}] role={role}, content_types={types}")
            else:
                logger.debug(f"  Converted [{i}] role={role}, content_preview={str(content)[:100]}...")

        # Build request kwargs
        request_kwargs: dict[str, Any] = {
            "model": model,
            "messages": anthropic_messages,
            "max_tokens": max_tokens,
            "temperature": temperature,
        }

        if anthropic_tools:
            request_kwargs["tools"] = anthropic_tools

        if top_k is not None:
            request_kwargs["top_k"] = top_k

        if thinking:
            request_kwargs["thinking"] = thinking

        if system:
            request_kwargs["system"] = system
        # Note: Removed the empty system string when enable_cache_headers is True
        # Cache headers should be added to specific message blocks, not as empty system

        try:
            response = await self.client.messages.create(**request_kwargs)
            return self._parse_response(response)
        except AuthenticationError as e:
            logger.error(f"Anthropic authentication error: {e}", exc_info=True)
            return LLMResponse(
                content="Anthropic API authentication failed. Check your API key.",
                finish_reason="error",
            )
        except RateLimitError as e:
            logger.error(f"Anthropic rate limit error: {e}", exc_info=True)
            return LLMResponse(
                content="Anthropic API rate limit exceeded. Please try again later.",
                finish_reason="error",
            )
        except APITimeoutError as e:
            logger.error(f"Anthropic timeout error: {e}", exc_info=True)
            return LLMResponse(
                content=f"Anthropic API timeout after {self.timeout}s",
                finish_reason="error",
            )
        except APIError as e:
            logger.error(f"Anthropic API error: {e}", exc_info=True)
            return LLMResponse(
                content=f"Anthropic API error: {e.message}",
                finish_reason="error",
            )
        except Exception as e:
            logger.exception("Unexpected Anthropic API error")
            return LLMResponse(
                content=f"Error calling Anthropic API: {str(e)}",
                finish_reason="error",
            )

    def _convert_tools_to_anthropic(self, tools: list[dict[str, Any]]) -> list[dict[str, Any]]:
        """
        Convert OpenAI-style tools to Anthropic format.

        Args:
            tools: List of tool definitions in OpenAI format.

        Returns:
            List of tool definitions in Anthropic format.
        """
        anthropic_tools = []
        for tool in tools:
            if "function" in tool:
                func = tool["function"]
                anthropic_tools.append({
                    "name": func["name"],
                    "description": func.get("description", ""),
                    "input_schema": func.get("parameters", {"type": "object"}),
                })
            else:
                # Already in Anthropic format
                anthropic_tools.append(tool)
        return anthropic_tools

    def _convert_content_blocks(self, content: list[dict[str, Any]]) -> list[dict[str, Any]]:
        """
        Convert OpenAI-style content blocks to Anthropic format.
        
        Converts:
        - {"type": "text", "text": "..."} -> same (compatible)
        - {"type": "image_url", "image_url": {"url": "data:mime;base64,..."}} 
          -> {"type": "image", "source": {"type": "base64", "media_type": "...", "data": "..."}}
        
        Args:
            content: List of content blocks in OpenAI format.
            
        Returns:
            List of content blocks in Anthropic format.
        """
        anthropic_content = []
        for block in content:
            block_type = block.get("type", "")
            
            if block_type == "text":
                anthropic_content.append(block)  # Compatible format
            elif block_type == "image_url":
                # Convert OpenAI image_url to Anthropic image format
                image_url = block.get("image_url", {}).get("url", "")
                if image_url.startswith("data:"):
                    # Parse data URL: data:mime/type;base64,<data>
                    try:
                        # Extract mime type and base64 data
                        header, b64_data = image_url.split(",", 1)
                        media_type = header.split(":")[1].split(";")[0]
                        anthropic_content.append({
                            "type": "image",
                            "source": {
                                "type": "base64",
                                "media_type": media_type,
                                "data": b64_data,
                            }
                        })
                    except (ValueError, IndexError) as e:
                        logger.warning(f"Failed to parse image data URL: {e}")
                else:
                    # URL-based image (not base64)
                    anthropic_content.append({
                        "type": "image",
                        "source": {
                            "type": "url",
                            "url": image_url,
                        }
                    })
            else:
                # Pass through unknown types
                anthropic_content.append(block)
        
        return anthropic_content

    def _convert_messages_to_anthropic(self, messages: list[dict[str, Any]]) -> list[dict[str, Any]]:
        """
        Convert messages from OpenAI format to Anthropic format.

        Handles:
        - System messages (extracted separately)
        - Tool calls and tool results
        - Multi-turn conversations

        Args:
            messages: List of message dicts in OpenAI format.

        Returns:
            List of message dicts in Anthropic format.
        """
        anthropic_messages = []
        # Collect consecutive tool results to batch them
        pending_tool_results = []
        
        for msg in messages:
            role = msg["role"]
            if role == "system":
                # Anthropic handles system separately, skip here
                continue
            
            # If we have pending tool results and hit a non-tool message, flush them
            if pending_tool_results and role != "tool":
                anthropic_messages.append({
                    "role": "user",
                    "content": pending_tool_results,
                })
                pending_tool_results = []
            
            if role == "assistant":
                content = msg.get("content", "")
                # Check for tool_calls
                tool_calls = msg.get("tool_calls")
                if tool_calls:
                    # Convert to Anthropic tool_use format
                    # IMPORTANT: text must come BEFORE tool_use blocks (Anthropic requirement)
                    anthropic_content = []
                    if content:
                        anthropic_content.append({"type": "text", "text": content})
                    for tc in tool_calls:
                        # Parse arguments - may be JSON string or dict
                        args = tc["function"].get("arguments", {})
                        if isinstance(args, str):
                            import json
                            try:
                                args = json.loads(args)
                            except json.JSONDecodeError:
                                args = {}
                        anthropic_content.append({
                            "type": "tool_use",
                            "id": tc["id"],
                            "name": tc["function"]["name"],
                            "input": args,
                        })
                    anthropic_messages.append({
                        "role": "assistant",
                        "content": anthropic_content,
                    })
                else:
                    anthropic_messages.append({"role": "assistant", "content": content})
            elif role == "tool":
                # OpenAI format: {"role": "tool", "tool_call_id": "...", "content": "..."}
                # Anthropic format: {"role": "user", "content": [{"type": "tool_result", "tool_use_id": "...", "content": "..."}]}
                pending_tool_results.append({
                    "type": "tool_result",
                    "tool_use_id": msg.get("tool_call_id"),
                    "content": msg.get("content", ""),
                })
            elif role == "user":
                # Check for tool results
                tool_results = []
                content = msg.get("content", "")
                if "tool_results" in msg:
                    for tr in msg["tool_results"]:
                        tool_results.append({
                            "type": "tool_result",
                            "tool_use_id": tr["tool_use_id"],
                            "content": tr.get("content", ""),
                        })
                    if tool_results:
                        anthropic_messages.append({
                            "role": "user",
                            "content": tool_results,
                        })
                else:
                    # Handle multi-part content (text + images)
                    if isinstance(content, list):
                        anthropic_content = self._convert_content_blocks(content)
                        anthropic_messages.append({"role": "user", "content": anthropic_content})
                    else:
                        anthropic_messages.append({"role": "user", "content": content})
        
        # Flush any remaining pending tool results
        if pending_tool_results:
            anthropic_messages.append({
                "role": "user",
                "content": pending_tool_results,
            })
        
        return anthropic_messages

    def _parse_response(self, response: Any) -> LLMResponse:
        """
        Parse Anthropic response into our standard format.

        Args:
            response: Raw response from Anthropic API.

        Returns:
            LLMResponse with parsed content, tool calls, and metadata.
        """
        tool_calls = []

        # Parse content blocks
        for block in response.content:
            if block.type == "tool_use":
                tool_calls.append(ToolCallRequest(
                    id=block.id,
                    name=block.name,
                    arguments=block.input,
                ))

        # Build text content from text blocks
        text_content = None
        text_blocks = [b.text for b in response.content if b.type == "text"]
        if text_blocks:
            text_content = "".join(text_blocks)

        # Parse usage
        usage = {}
        if hasattr(response, "usage") and response.usage:
            usage = {
                "prompt_tokens": response.usage.input_tokens,
                "completion_tokens": response.usage.output_tokens,
                "total_tokens": response.usage.input_tokens + response.usage.output_tokens,
            }

        # Map Anthropic stop_reason to our format
        finish_reason = response.stop_reason or "stop"

        return LLMResponse(
            content=text_content,
            tool_calls=tool_calls,
            finish_reason=finish_reason,
            usage=usage,
        )

    def get_default_model(self) -> str:
        """Get the default model."""
        return self.default_model
